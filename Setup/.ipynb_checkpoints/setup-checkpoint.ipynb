{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###---Some general imports---###\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "\n",
    "from glob import glob\n",
    "from astropy.io import ascii\n",
    "import bessels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performing some initial precomputations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This snippet rearranges the EFTCAMB output files for ease of use with the Fortran integrator.\n",
    "# Download the required files from https://www.dropbox.com/sh/m1lrzdjxwp3ioyl/AADRHWZxE-Fn-Un-HBApEYxja?dl=0 \n",
    "# and unzip the contents in the /Data folder\n",
    "# Go grab a coffee, this might take about an hour!\n",
    "\n",
    "###---IN CASE YOU ARE IN A HURRY---###\n",
    "# We also provide the output of this snippet. \n",
    "# In case you would like to use them, simply download them from \n",
    "# https://www.dropbox.com/sh/to40ws3amzammb5/AACAGU7P03fP473uXfxyKlfua?dl=0\n",
    "# Unzip the contents into /Data/grid folder.\n",
    "# Skip this snippet altogether.\n",
    "\n",
    "\n",
    "\n",
    "ell_lst = np.arange(10, 100)\n",
    "\n",
    "for counter, filename in enumerate(glob('../Data/EFT_CAMB_output/*.dat')):\n",
    "    print counter, filename \n",
    "    \n",
    "    newfilename =  filename.strip('../Data/EFT_CAMB_output/').strip('_cache_MetricMG.dat')\n",
    "    Fortran_output = ascii.read(filename)\n",
    "    #Let's cut out the very large z's.\n",
    "    Fortran_output = Fortran_output[(1./Fortran_output['a'].data - 1.) < 7.]\n",
    "\n",
    "    #Separate columns\n",
    "    a_lst = Fortran_output['a'].data\n",
    "    eta_lst = Fortran_output['tau'].data\n",
    "    k_lst = Fortran_output['k'].data\n",
    "    confH_lst = Fortran_output['adotoa'].data\n",
    "    sigma_lst = Fortran_output['sigma'].data\n",
    "    deltaCDM_lst = Fortran_output['clxc'].data\n",
    "\n",
    "    #The unique values of k and eta\n",
    "    k_unique_lst = np.unique(k_lst)\n",
    "    eta_unique_lst = np.unique(eta_lst)\n",
    "\n",
    "    #The number of unique 'k' and 'eta' values\n",
    "    k_length = len(k_unique_lst)\n",
    "    eta_length = len(eta_unique_lst)\n",
    "\n",
    "    #Note that the number of unique 'a' values is not the same as the num of 'eta' unique values \n",
    "    a_unique_lst = [ a_lst[np.where(eta_lst == eta_unique)[0][0]] for eta_unique in eta_unique_lst]\n",
    "\n",
    "\n",
    "    #Finally, let's reshape in the form of k_length x eta_length\n",
    "    confH = np.reshape(confH_lst, (k_length, eta_length))\n",
    "    sigma = np.reshape(sigma_lst, (k_length, eta_length))\n",
    "    deltaCDM = np.reshape(deltaCDM_lst, (k_length, eta_length))\n",
    "\n",
    "    #We define this array for using in Bessel function calculations\n",
    "    keta_lst = np.kron(k_unique_lst, (max(eta_unique_lst) - eta_unique_lst))\n",
    "\n",
    "    num_x = 100000\n",
    "    max_x = max(k_unique_lst)*(max(eta_unique_lst) - min(eta_unique_lst))\n",
    "    x = np.linspace(0., max_x, num_x)\n",
    "\n",
    "    k_Recompute_index = 50\n",
    "    ell_Recompute = len(ell_lst)\n",
    "\n",
    "    max_x_Recompute = k_unique_lst[k_Recompute_index]*(max(eta_unique_lst) - min(eta_unique_lst))\n",
    "    x_Recompute = np.linspace(0., max_x_Recompute, num_x)\n",
    "\n",
    "\n",
    "    bess_int_keta_lst, bessPrime_int_keta_lst = \\\n",
    "        bessels.bessel_calculation(1, ell_lst, x, x_Recompute, ell_Recompute, keta_lst, k_Recompute_index)\n",
    "\n",
    "    bess_int_keta_reshaped = np.reshape(bess_int_keta_lst, (len(ell_lst), k_length, eta_length))\n",
    "    bessPrime_int_keta_reshaped = np.reshape(bessPrime_int_keta_lst, (len(ell_lst), k_length, eta_length))\n",
    "\n",
    "\n",
    "    np.save('../Data/grid/F/'+newfilename+'.npy', [a_unique_lst, k_unique_lst, eta_unique_lst, \\\n",
    "                                              bess_int_keta_reshaped, confH, sigma, \\\n",
    "                                              deltaCDM, k_length, eta_length])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# This snippet rearranges the EFTCAMB output files for ease of use with the Python integrator.\n",
    "###---The Python integrator is completely equivalent to the Fortran integrator in terms of accuracy.\n",
    "###---It is, however, somewhat slower.\n",
    "# Download the required files from https://www.dropbox.com/sh/m1lrzdjxwp3ioyl/AADRHWZxE-Fn-Un-HBApEYxja?dl=0 \n",
    "# Unzip the contents in the /Data folder.\n",
    "# Go grab a coffee, this might take about an hour!\n",
    "\n",
    "from glob import glob\n",
    "from astropy.table import Table\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.special import spherical_jn\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.integrate import quad, simps, trapz\n",
    "from math import factorial\n",
    "from time import time\n",
    "\n",
    "for counter, filename in enumerate(glob('../Data/EFT_CAMB_output/*.dat')):\n",
    "        \n",
    "    print counter, filename\n",
    "    newfilename =  filename.strip('../Data/EFT_CAMB_output/').strip('_cache_MetricMG.dat')\n",
    "    \n",
    "    \n",
    "    data = Table.read(filename, format=\"ascii\")\n",
    "    \n",
    "    \n",
    "    # Interpolation points for the k*dtau \n",
    "    N_points = 200000\n",
    "\n",
    "    # How many l's?\n",
    "    l_list = np.arange(10, 100)\n",
    "\n",
    "\n",
    "    # unique k, tau arrays\n",
    "    k_unique = np.unique(data['k'])\n",
    "    tau_unique = np.unique(data['tau'])\n",
    "    tau_max = tau_unique.max()\n",
    "\n",
    "    N_tau = len(tau_unique)\n",
    "    N_k = len(k_unique)\n",
    "    N_l = len(l_list)\n",
    "\n",
    "\n",
    "\n",
    "    x_int_max = k_unique[-1]*tau_unique[-1]*1.1\n",
    "\n",
    "\n",
    "    l_list_small = l_list[l_list<5]\n",
    "    N_l_small = (l_list<5).sum()\n",
    "\n",
    "    print \"N_tau, N_k, N_l are\", N_tau, N_k, N_l    \n",
    "    \n",
    "    k = np.tile(data['k'].quantity.value.reshape(N_k, N_tau), (N_l, 1, 1))\n",
    "    tau = np.tile(data['tau'].quantity.value.reshape(N_k, N_tau), (N_l, 1, 1))\n",
    "    H = np.tile(data['adotoa'].quantity.value.reshape(N_k, N_tau), (N_l, 1, 1))\n",
    "    clxc = np.tile(data['clxc'].quantity.value.reshape(N_k, N_tau), (N_l, 1, 1))\n",
    "    sigma = np.tile(data['sigma'].quantity.value.reshape(N_k, N_tau), (N_l, 1, 1))\n",
    "    \n",
    "\n",
    "    j_int = np.zeros((N_l, N_tau*N_k))\n",
    "    j_p_int = np.zeros((N_l, N_tau*N_k))\n",
    "    x_int = np.linspace(0, x_int_max, N_points)\n",
    "    dx = x_int[2]-x_int[1]    \n",
    "    \n",
    "    \n",
    "    \n",
    "    for i in xrange(N_l):\n",
    "        y_int = spherical_jn(l_list[i], x_int)\n",
    "        #y_int_c = (y_int[1:]+y_int[:-1])*(x_int[1:] - x_int[:-1])/2. \n",
    "        y_int_c = np.cumsum(y_int)*dx\n",
    "        j_int[i, :] =  -np.interp( (data['k']*(tau_max-data['tau'])).quantity.value, x_int, y_int_c)/data['k'].quantity.value\n",
    "        j_p_int[i, :] = -np.interp( (data['k']*(tau_max-data['tau'])).quantity.value, x_int, y_int)/data['k'].quantity.value\n",
    "\n",
    "    j_int_old = np.copy(j_int)\n",
    "    j_int_old = np.reshape(j_int_old, (N_l, N_k, N_tau))\n",
    "    j_int_old = np.gradient(j_int_old, axis=2)\n",
    "\n",
    "\n",
    "    print \"super grid\"\n",
    "    x_sl = (data['k']*(tau_max-data['tau'])).quantity.value\n",
    "    k_sl = data['k'].quantity.value\n",
    "\n",
    "    x_int_max_2 = x_int_max/N_points*1000.\n",
    "    x_int_2 = np.linspace(0, x_int_max_2, N_points)\n",
    "    dx =  x_int_2[2]-x_int_2[1]\n",
    "\n",
    "    idx = x_sl < x_int_max_2\n",
    "\n",
    "    for i in xrange(N_l_small):\n",
    "        y_int = spherical_jn(l_list_small[i], x_int_2) \n",
    "        y_int_c = np.cumsum(y_int)*dx\n",
    "        j_int[i, idx] =  -np.interp( x_sl[idx], x_int_2, y_int_c)/k_sl[idx]\n",
    "        j_p_int[i, idx] = -np.interp( x_sl[idx], x_int_2, y_int)/k_sl[idx]\n",
    "\n",
    "\n",
    "    j_int = np.reshape(j_int, (N_l, N_k, N_tau))\n",
    "    j_p_int = np.reshape(j_p_int, (N_l, N_k, N_tau))\n",
    "    j_int_temp = np.diff(j_int, axis=2)\n",
    "    j_p_int_temp = np.diff(j_p_int, axis=2)\n",
    "\n",
    "    j_int[:, :, :-1] = j_int_temp\n",
    "    j_p_int[:, :, :-1] = j_p_int_temp\n",
    "\n",
    "    j_int[:, :, -1] = 0\n",
    "    j_p_int[:, :, -1] = 0\n",
    "    \n",
    "    \n",
    "    a_unique = data['a'].quantity.value.reshape(N_k, N_tau)[0]\n",
    "    np.save('../Data/grid/P/'+newfilename+'.npy', [a_unique, data['clxc'].quantity.value, j_int, data['adotoa'].quantity.value, data['sigma'].quantity.value, data['k'].quantity.value, N_k, N_l, N_tau])\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performing the necessary precomputations for a range of $k_\\mathrm{max}$ values, for $\\Omega_\\mathrm{m} \\approx 0.32$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ell_lst = np.arange(10, 500)\n",
    "\n",
    "Om_baryons = 0.0493\n",
    "num_Om_M = 60\n",
    "Om_M_lst = np.round(np.linspace(0.1, 0.7, num_Om_M), 4)\n",
    "Om_indx = np.abs(0.32 - Om_M_lst).argmin() #Om approx 0.32\n",
    "\n",
    "filename = '../Data/EFT_CAMB_output/' + str(Om_indx) + '_cache_MetricMG.dat'\n",
    "Fortran_output = ascii.read(filename)\n",
    "\n",
    "#print np.max(Fortran_output['k'].data)\n",
    "\n",
    "k_max_lst = np.arange(0.32, 0.08, -0.02)\n",
    "\n",
    "for indx, k_max_tmp in enumerate(k_max_lst):\n",
    "    \n",
    "    print indx + 1, k_max_tmp\n",
    "    \n",
    "    #Let's cut out the very large z's.\n",
    "    Fortran_output = Fortran_output[(1./Fortran_output['a'].data - 1.) < 7.]\n",
    "    \n",
    "    #Let's cut out the k's.\n",
    "    Fortran_output = Fortran_output[Fortran_output['k'].data < k_max_tmp]\n",
    "    \n",
    "    #Separate columns\n",
    "    a_lst = Fortran_output['a'].data\n",
    "    eta_lst = Fortran_output['tau'].data\n",
    "    k_lst = Fortran_output['k'].data\n",
    "    confH_lst = Fortran_output['adotoa'].data\n",
    "    sigma_lst = Fortran_output['sigma'].data\n",
    "    deltaCDM_lst = Fortran_output['clxc'].data\n",
    "    \n",
    "    #The unique values of k and eta\n",
    "    k_unique_lst = np.unique(k_lst)\n",
    "    eta_unique_lst = np.unique(eta_lst)\n",
    "\n",
    "    #The number of unique 'k' and 'eta' values\n",
    "    k_length = len(k_unique_lst)\n",
    "    eta_length = len(eta_unique_lst)\n",
    "\n",
    "    #Note that the number of unique 'a' values is not the same as the num of 'eta' unique values \n",
    "    a_unique_lst = [ a_lst[np.where(eta_lst == eta_unique)[0][0]] for eta_unique in eta_unique_lst]\n",
    "\n",
    "\n",
    "    #Finally, let's reshape in the form of k_length x eta_length\n",
    "    confH = np.reshape(confH_lst, (k_length, eta_length))\n",
    "    sigma = np.reshape(sigma_lst, (k_length, eta_length))\n",
    "    deltaCDM = np.reshape(deltaCDM_lst, (k_length, eta_length))\n",
    "\n",
    "    #We define this array for using in Bessel function calculations\n",
    "    keta_lst = np.kron(k_unique_lst, (max(eta_unique_lst) - eta_unique_lst))\n",
    "\n",
    "    num_x = 100000\n",
    "    max_x = max(k_unique_lst)*(max(eta_unique_lst) - min(eta_unique_lst))\n",
    "    x = np.linspace(0., max_x, num_x)\n",
    "\n",
    "    k_Recompute_index = 50\n",
    "    ell_Recompute = len(ell_lst)\n",
    "\n",
    "    max_x_Recompute = k_unique_lst[k_Recompute_index]*(max(eta_unique_lst) - min(eta_unique_lst))\n",
    "    x_Recompute = np.linspace(0., max_x_Recompute, num_x)\n",
    "\n",
    "\n",
    "    bess_int_keta_lst, bessPrime_int_keta_lst = \\\n",
    "        bessels.bessel_calculation(1, ell_lst, x, x_Recompute, ell_Recompute, keta_lst, k_Recompute_index)\n",
    "\n",
    "    bess_int_keta_reshaped = np.reshape(bess_int_keta_lst, (len(ell_lst), k_length, eta_length))\n",
    "    bessPrime_int_keta_reshaped = np.reshape(bessPrime_int_keta_lst, (len(ell_lst), k_length, eta_length))\n",
    "\n",
    "\n",
    "    np.save('../Data/grid/F/k_range_tests/' + str(indx + 1) + '.npy', [a_unique_lst, k_unique_lst, eta_unique_lst, \\\n",
    "                                              bess_int_keta_reshaped, confH, sigma, \\\n",
    "                                              deltaCDM, k_length, eta_length])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
